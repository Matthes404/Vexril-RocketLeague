# Training Configuration for Rocket League RL Bot

# Environment settings (RLGym-Sim)
env:
  tick_skip: 8  # Number of physics ticks to skip per action
  spawn_opponents: true  # Whether to spawn opponent bots
  team_size: 1  # 1v1, 2v2, or 3v3
  timeout_seconds: 300  # Max episode length in seconds

  # Opponent policy for training (IMPORTANT for learning to play against real opponents!)
  # Options: "zero" (frozen - NOT recommended), "random", "chase", "mixed" (default)
  # "mixed" switches between random, chase, and occasionally zero actions
  # This helps the bot learn to handle different opponent behaviors
  opponent_policy: "mixed"

# PPO hyperparameters
ppo:
  learning_rate: 0.0003  # Learning rate for the optimizer
  n_steps: 4096  # Number of steps to collect per environment per update
  batch_size: 256  # Minibatch size for each gradient update
  n_epochs: 10  # Number of epochs to train on each collected batch
  gamma: 0.995  # Discount factor (increased from 0.99 for longer-term planning)
  gae_lambda: 0.95  # GAE lambda parameter
  clip_range: 0.2  # Clipping parameter for PPO
  ent_coef: 0.02  # Entropy coefficient (INCREASED from 0.01 for more exploration early on)
  vf_coef: 1.0  # Value function coefficient (INCREASED from 0.5 to prioritize value learning)
  max_grad_norm: 0.5  # Maximum gradient norm for clipping

# Training settings
training:
  total_timesteps: 10_000_000  # Total number of training timesteps
  save_freq: 100_000  # Save model every N steps
  model_name: "rl_bot"  # Base name for saved models
  resume_training: false  # Whether to resume from existing checkpoint

# Paths
paths:
  models_dir: "./models"  # Directory to save trained models
  logs_dir: "./logs"  # Directory for tensorboard logs
