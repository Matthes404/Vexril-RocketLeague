# Training Configuration for Rocket League RL Bot

# Environment settings (RLGym-Sim)
env:
  num_envs: 8  # Number of parallel environments for faster training
  tick_skip: 8  # Number of physics ticks to skip per action
  spawn_opponents: true  # Whether to spawn opponent bots
  team_size: 1  # 1v1, 2v2, or 3v3
  timeout_seconds: 300  # Max episode length in seconds

  # Self-play mode (RECOMMENDED for 1v1!)
  # When true, all agents use the model's policy and ALL training data is used
  # This doubles training efficiency in 1v1 (uses both agents' experiences)
  self_play: true

  # Opponent policy (only used when self_play: false)
  # Options: "self" (same as self_play: true), "random", "chase", "mixed", "zero"
  opponent_policy: "self"

# PPO hyperparameters (optimized for 8 parallel environments)
ppo:
  learning_rate: 0.0001  # Learning rate (lower for stability with more envs)
  n_steps: 2048  # Steps per env per update → 2048 × 8 = 16,384 total
  batch_size: 2048  # Larger batches for more stable gradients
  n_epochs: 4  # Fewer epochs (more data per update)
  gamma: 0.995  # Discount factor (increased from 0.99 for longer-term planning)
  gae_lambda: 0.95  # GAE lambda parameter
  clip_range: 0.2  # Clipping parameter for PPO
  ent_coef: 0.01  # Entropy coefficient for exploration
  vf_coef: 1.0  # Value function coefficient (INCREASED from 0.5 to prioritize value learning)
  max_grad_norm: 0.5  # Maximum gradient norm for clipping

# Training settings
training:
  total_timesteps: 10_000_000  # Total number of training timesteps
  save_freq: 100_000  # Save model every N steps
  model_name: "rl_bot"  # Base name for saved models
  resume_training: false  # Whether to resume from existing checkpoint

# Paths
paths:
  models_dir: "./models"  # Directory to save trained models
  logs_dir: "./logs"  # Directory for tensorboard logs
