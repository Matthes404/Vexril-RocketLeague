# Training Configuration for Rocket League RL Bot

# Environment settings (RLGym-Sim)
env:
  tick_skip: 8  # Number of physics ticks to skip per action
  spawn_opponents: true  # Whether to spawn opponent bots
  team_size: 1  # 1v1, 2v2, or 3v3
  timeout_seconds: 300  # Max episode length in seconds

  # Self-play mode (RECOMMENDED for 1v1!)
  # When true, all agents use the model's policy and ALL training data is used
  # This doubles training efficiency in 1v1 (uses both agents' experiences)
  self_play: true

  # Opponent policy (only used when self_play: false)
  # Options: "self" (same as self_play: true), "random", "chase", "mixed", "zero"
  opponent_policy: "self"

# PPO hyperparameters
ppo:
  learning_rate: 0.0003  # Learning rate for the optimizer
  n_steps: 4096  # Number of steps to collect per environment per update
  batch_size: 256  # Minibatch size for each gradient update
  n_epochs: 10  # Number of epochs to train on each collected batch
  gamma: 0.995  # Discount factor (increased from 0.99 for longer-term planning)
  gae_lambda: 0.95  # GAE lambda parameter
  clip_range: 0.2  # Clipping parameter for PPO
  ent_coef: 0.02  # Entropy coefficient (INCREASED from 0.01 for more exploration early on)
  vf_coef: 1.0  # Value function coefficient (INCREASED from 0.5 to prioritize value learning)
  max_grad_norm: 0.5  # Maximum gradient norm for clipping

# Training settings
training:
  total_timesteps: 10_000_000  # Total number of training timesteps
  save_freq: 100_000  # Save model every N steps
  model_name: "rl_bot"  # Base name for saved models
  resume_training: false  # Whether to resume from existing checkpoint

# Paths
paths:
  models_dir: "./models"  # Directory to save trained models
  logs_dir: "./logs"  # Directory for tensorboard logs
